{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Music Genre Classification with Deep Learning\n",
    "\n",
    "This tutorial shows how different Convolutional Neural Network architectures are used for the taks of music genre classification.\n",
    "\n",
    "The data set used is the [GTZAN](http://marsyasweb.appspot.com/download/data_sets/) genre data set compiled by George Tzanetakis. It consists of 1000 tracks (30 second excerpts) from 10 genres, each with 100 examples.\n",
    "\n",
    "The original tracks are 22050Hz Mono 16-bit audio files in .au format.\n",
    "\n",
    "For a more compact download we provide a version in .mp3 format, also 22050 Hz.\n",
    "\n",
    "This tutorial contains:\n",
    "* Loading and preprocessing of audio files\n",
    "* Loading class files from CSV and using Label Encoder\n",
    "* using One-Hot Encoder to prepare class data for Deep Learning\n",
    "* Generating Mel spectrograms from the audio\n",
    "* Standardization of data\n",
    "* Train/Test set split\n",
    "* Stratified splits\n",
    "* Convolutional Neural Networks: single, stacked, parallel\n",
    "* ReLU Activation\n",
    "* Dropout\n",
    "* Batch Normalization\n",
    "\n",
    "(entensions for later:)\n",
    "* Majority Vote / Max Probability \n",
    "* Recurrent Neural Networks\n",
    "* Cross-validation\n",
    "\n",
    "You can execute the following code blocks by pressing SHIFT+Enter consecutively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# choosing between CPU and GPU\n",
    "#device = 'cpu' # 'cpu' or 'gpu'\n",
    "#os.environ['THEANO_FLAGS']='mode=FAST_RUN,device=' + device + ',floatX=float32'\n",
    "\n",
    "import argparse\n",
    "import csv\n",
    "import datetime\n",
    "import glob\n",
    "import math\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd # Pandas for reading CSV files and easier Data handling in preparation\n",
    "from os.path import join\n",
    "\n",
    "from theano import config\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Convolution2D, MaxPooling2D, Dense, Dropout, Activation, Flatten, merge\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "# local\n",
    "import rp_extract as rp\n",
    "from audiofile_read import audiofile_read\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import __version__ as sklearn_version\n",
    "\n",
    "if sklearn_version.startswith('0.17'):\n",
    "    from sklearn.cross_validation import train_test_split\n",
    "    from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "else: # >= 0.18\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.model_selection import StratifiedShuffleSplit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set the Path to the Dataset\n",
    "\n",
    "adjust this path to where the data set is stored on your computer:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# SET YOUR OWN PATH HERE\n",
    "AUDIO_PATH = '../data/GTZAN_mp3'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Metadata\n",
    "\n",
    "The tab-separated file contains pairs of filename TAB class category (i.e. genre)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>./rock/rock.00053.mp3</th>\n",
       "      <td>rock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>./rock/rock.00051.mp3</th>\n",
       "      <td>rock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>./rock/rock.00076.mp3</th>\n",
       "      <td>rock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>./rock/rock.00084.mp3</th>\n",
       "      <td>rock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>./rock/rock.00052.mp3</th>\n",
       "      <td>rock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>./rock/rock.00057.mp3</th>\n",
       "      <td>rock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>./rock/rock.00028.mp3</th>\n",
       "      <td>rock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>./rock/rock.00035.mp3</th>\n",
       "      <td>rock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>./rock/rock.00095.mp3</th>\n",
       "      <td>rock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>./rock/rock.00088.mp3</th>\n",
       "      <td>rock</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          1\n",
       "0                          \n",
       "./rock/rock.00053.mp3  rock\n",
       "./rock/rock.00051.mp3  rock\n",
       "./rock/rock.00076.mp3  rock\n",
       "./rock/rock.00084.mp3  rock\n",
       "./rock/rock.00052.mp3  rock\n",
       "./rock/rock.00057.mp3  rock\n",
       "./rock/rock.00028.mp3  rock\n",
       "./rock/rock.00035.mp3  rock\n",
       "./rock/rock.00095.mp3  rock\n",
       "./rock/rock.00088.mp3  rock"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_file = join(AUDIO_PATH,'filelist_GTZAN_mp3_wclasses.txt')\n",
    "metadata = pd.read_csv(csv_file, index_col=0, header=None)\n",
    "metadata.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create list of filenames with associated classes\n",
    "filelist = metadata.index.tolist()\n",
    "classes = metadata[1].values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode Labels to Numbers\n",
    "\n",
    "String labels need to be encoded as numbers. We use the LabelEncoder from the scikit-learn package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rock', 'rock', 'rock', 'rock', 'rock']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rock', 'hiphop', 'hiphop', 'hiphop', 'hiphop', 'hiphop']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes[99:105]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 classes: blues, classical, country, disco, hiphop, jazz, metal, pop, reggae, rock\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "labelencoder = LabelEncoder()\n",
    "labelencoder.fit(classes)\n",
    "\n",
    "# we keep (and print) the number of classis\n",
    "n_classes = len(labelencoder.classes_)\n",
    "print n_classes, \"classes:\", \", \".join(list(labelencoder.classes_))\n",
    "\n",
    "classes_num = labelencoder.transform(classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We check how the classes look like now numerically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9, 9, 9, 9, 9])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes_num[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9, 4, 4, 4, 4, 4])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes_num[99:105]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: In order to correctly re-transform any predicted numbers into strings, we keep the labelencoder for later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.],\n",
       "       ..., \n",
       "       [ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# make a row vector a column vector, as needed by OneHotEncoder, using reshape(-1,1) \n",
    "classes_num_col = classes_num.reshape(-1, 1)\n",
    "\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "classes_num_1hot = encoder.fit_transform(classes_num_col)\n",
    "classes_num_1hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 10)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes_num_1hot.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Audio Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . \n",
      "Read 1000 audio files\n"
     ]
    }
   ],
   "source": [
    "\n",
    "list_spectrograms = [] # spectrograms are put into a list first\n",
    "\n",
    "# desired output parameters\n",
    "n_mel_bands = 40   # y axis\n",
    "frames = 80        # x axis\n",
    "\n",
    "# some FFT parameters\n",
    "fft_window_size=512\n",
    "fft_overlap = 0.5\n",
    "hop_size = int(fft_window_size*(1-fft_overlap))\n",
    "segment_size = fft_window_size + (frames-1) * hop_size # segment size for desired # frames\n",
    "\n",
    "for filename in filelist:\n",
    "    print \".\", \n",
    "    filepath = os.path.join(AUDIO_PATH, filename)\n",
    "    samplerate, samplewidth, wavedata = audiofile_read(filepath,verbose=False)\n",
    "    sample_length = wavedata.shape[0]\n",
    "\n",
    "    # make Mono (in case of multiple channels / stereo)\n",
    "    if wavedata.ndim > 1:\n",
    "        wavedata = np.mean(wavedata, 1)\n",
    "      \n",
    "    # GET AUDIO SEGMENT (BLOCK)\n",
    "    # take only 1 audio segment of 0.94 sec\n",
    "    # a) from beginning of file\n",
    "    #pos = 0\n",
    "    # b) from middle of file\n",
    "    pos = int(sample_length / 2 - segment_size / 2)\n",
    "    wav_segment = wavedata[pos:pos+segment_size]\n",
    "    \n",
    "    # AUDIO PRE-PROCESSING\n",
    "\n",
    "    # 1) FFT spectrogram \n",
    "    spectrogram = rp.calc_spectrogram(wav_segment,fft_window_size,fft_overlap)\n",
    "\n",
    "    # 2) Transform to perceptual Mel scale (uses librosa.filters.mel)\n",
    "    spectrogram = rp.transform2mel(spectrogram,samplerate,fft_window_size,n_mel_bands)\n",
    "        \n",
    "    # 3) Log 10 transform\n",
    "    spectrogram = np.log10(spectrogram)\n",
    "    \n",
    "    list_spectrograms.append(spectrogram)\n",
    "        \n",
    "print \"\\nRead\", len(filelist), \"audio files\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duration of input audio file:\n",
      "30 seconds\n"
     ]
    }
   ],
   "source": [
    "print \"Duration of input audio file:\"\n",
    "print sample_length / samplerate, \"seconds\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An audio segment is 0.94 seconds long\n"
     ]
    }
   ],
   "source": [
    "print \"An audio segment is\", round(float(segment_size) / samplerate, 2), \"seconds long\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how many spectrograms did we get\n",
    "len(list_spectrograms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40, 80)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# whats the shape of 1 spectrogram: 40 Mel bands by 80 frames\n",
    "spectrogram.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: For simplicity of this tutorial, here we load only 1 single segment of ~ 1 second length from each audio file.\n",
    "In a real setting, one would create training instances of as many audio segments as possible to be fed to a Neural Network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-3.19312881, -2.46611857, -2.33069044, -2.78888361, -2.65921513],\n",
       "       [-2.81906141, -2.14093101, -2.20024035, -2.72672749, -2.35348157],\n",
       "       [-1.95557674, -1.92403767, -2.03611306, -2.07349593, -1.97247792],\n",
       "       [-2.02953207, -2.05230601, -1.98668831, -2.02436922, -2.00982927],\n",
       "       [-2.09251568, -2.77670719, -2.43870339, -2.29348992, -3.07332277]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spectrogram[0:5,0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO plot spectrogram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make 1 big array of list of spectrograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 40, 80)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a list of many 40x80 spectrograms is made into 1 big array\n",
    "# config.floatX is from Theano configration to enforce float32 precision (needed for GPU computation)\n",
    "data = np.array(list_spectrograms, dtype=config.floatX)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# check for Inf values\n",
    "\n",
    "# np.log10(spectrogram) will produce -inf if a spectrogram value is 0. we replace -inf by 0 here\n",
    "\n",
    "if np.any(np.isinf(data)):\n",
    "    print \"Warning: Data contains inf values. Replacying by 0.\"\n",
    "    data[np.isinf(data)] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardization\n",
    "\n",
    "<b>Always normalize / standardize</b> the data before feeding it into the Neural Network!\n",
    "\n",
    "We use <b>attribute-wise standardization</b>, i.e. each feature (i.e. 'pixel' in the spectrogram) is standardized individually, as opposed to computing a single mean and single standard deviation of all values.\n",
    "\n",
    "(Instead of 'attribute-wise, also 'flat' standardization would also be possible,computing the mean and standard deviation across all pixels).\n",
    "\n",
    "One possibility is 'Min-Max normalization', i.e. scaling the values between 0 and 1.\n",
    "\n",
    "Here we use <b>Zero-mean Unit-variance standardization</b> (also known as Z-score normalization).\n",
    "\n",
    "We use the StandardScaler from the scikit-learn package for our purpose, which performs a Zero-mean Unit-variance standardization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "need more than 2 values to unpack",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-54d15a0b5391>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# vectorize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mydim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxdim\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mydim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: need more than 2 values to unpack"
     ]
    }
   ],
   "source": [
    "# Scalers and normalizers work on vectors. So we have to transfor the matrix of our spectrograms into vector data\n",
    "# ('vectorize' or 'reshape' them).\n",
    "\n",
    "# vectorize\n",
    "N, ydim, xdim = data.shape\n",
    "data = data.reshape(N, xdim*ydim)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 3200)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0:1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# standardize\n",
    "scaler = preprocessing.StandardScaler()\n",
    "data = scaler.fit_transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now all the values are transformed into the 0-mean 1-variance space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.        ,  0.        ,  0.00003291, -0.00000348, -0.00000135, -0.00000497, -0.0000019 ,  0.00000086, -0.00000034, -0.000001  , ..., -0.00000007,  0.00000116, -0.00000242, -0.00000083,\n",
       "       -0.00000178,  0.00000348,  0.00000149,  0.00000025, -0.00000021,  0.00000247], dtype=float32)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(data, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.        ,  0.        ,  1.00000322,  0.99999994,  0.99999982,  0.99999994,  1.        ,  1.        ,  1.        ,  1.00000024, ...,  1.00000012,  1.00000024,  1.00000024,  1.00000024,\n",
       "        1.        ,  0.99999988,  0.99999952,  1.        ,  0.99999964,  1.00000012], dtype=float32)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(data, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  0.        ,   0.        , -15.83200645, -13.3323307 , -11.67720318,  -5.16446209,  -3.19460917,  -3.1655395 ,  -3.19477248,  -3.2418499 , ...,  -8.34243298,  -8.35699749,  -8.34202576,\n",
       "         -8.32057381,  -8.31046009,  -8.28858852,  -8.29681206,  -8.29219341,  -8.30333328,  -8.30508804], dtype=float32),\n",
       " array([ 1.        ,  1.        ,  1.7693063 ,  1.36666048,  1.36915123,  1.21423578,  1.23309195,  1.27106905,  1.36281705,  1.50109375, ...,  2.0394578 ,  2.01078248,  2.03702593,  2.05707383,\n",
       "         2.05797529,  2.05667782,  2.06773233,  2.05173779,  2.05352616,  2.06124711], dtype=float32))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# scaler stores the original values to be able to transform later again\n",
    "# show mean and standard deviation: two vectors with same length as data.shape[1]\n",
    "scaler.mean_, scaler.scale_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Train & Test Set \n",
    "\n",
    "We split the original full data set into two parts: Train Set (75%) and Test Set (25%).\n",
    "\n",
    "Here we compare Random Split vs. Stratified Split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "testset_size = 0.25 # % portion of whole data set to keep for testing, i.e. 75% is used for training\n",
    "\n",
    "# RANDOM split of data set into 2 parts\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_set, test_set, train_classes, test_classes = train_test_split(data, classes_num, test_size=testset_size, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 8, 9, 2, 9, 7, 8, 5, 2, 9, ..., 7, 9, 6, 7, 7, 0, 4, 8, 1, 8])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 0, 3, 1, 8, 2, 9, 3, 6, 7, ..., 1, 5, 3, 6, 2, 5, 6, 9, 5, 8])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files in each category in TRAIN set:\n",
      "0 : 80\n",
      "1 : 74\n",
      "2 : 70\n",
      "3 : 70\n",
      "4 : 84\n",
      "5 : 72\n",
      "6 : 74\n",
      "7 : 74\n",
      "8 : 73\n",
      "9 : 79\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "cnt = Counter(train_classes)\n",
    "\n",
    "print \"Number of files in each category in TRAIN set:\"\n",
    "for k in sorted(cnt.keys()):\n",
    "    print k, \":\", cnt[k]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a Random Split, the number of files per class may be uneven or unbalanced.\n",
    "\n",
    "The better way to do it is to use a <b>Stratified Split</b>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN INDEX: [349 816 960 261 665 517 653 438 170 472 ..., 336 602 117  93 322 434 624  83 118  97]\n",
      "TEST INDEX: [510 781 478 246 888 793 772 445 199 534 ..., 967 490 486 683 984 390 858 372 982 499]\n",
      "(750, 3200)\n",
      "(250, 3200)\n"
     ]
    }
   ],
   "source": [
    "# better: Stratified Split retains the class balance in both sets\n",
    "# from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "if sklearn_version.startswith('0.17'):\n",
    "    splits = StratifiedShuffleSplit(classes_num, n_iter=1, test_size=testset_size, random_state=0)\n",
    "else: # >= 0.18:\n",
    "    splitter = StratifiedShuffleSplit(n_splits=1, test_size=testset_size, random_state=0)\n",
    "    splits = splitter.split(data, classes_num)\n",
    "\n",
    "for train_index, test_index in splits:\n",
    "    print \"TRAIN INDEX:\", train_index\n",
    "    print \"TEST INDEX:\", test_index\n",
    "    \n",
    "    # split the data\n",
    "    train_set = data[train_index]\n",
    "    test_set = data[test_index]\n",
    "    \n",
    "    # and the numeric classes (groundtruth)\n",
    "    train_classes = classes_num[train_index]\n",
    "    train_classes_1hot = classes_num_1hot[train_index]  # 1 hot we need for traning\n",
    "    test_classes = classes_num[test_index]\n",
    "# Note: this for loop is only executed once, if n_splits==1\n",
    "\n",
    "print train_set.shape\n",
    "print test_set.shape\n",
    "# Note: we will reshape the data later back to matrix form "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files in each category in TRAIN set:\n",
      "0 : 75\n",
      "1 : 75\n",
      "2 : 75\n",
      "3 : 75\n",
      "4 : 75\n",
      "5 : 75\n",
      "6 : 75\n",
      "7 : 75\n",
      "8 : 75\n",
      "9 : 75\n"
     ]
    }
   ],
   "source": [
    "cnt = Counter(train_classes)\n",
    "print \"Number of files in each category in TRAIN set:\"\n",
    "for k in sorted(cnt.keys()):\n",
    "    print k, \":\", cnt[k]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Now the number of files in each category in the training set is equal.\n",
    "\n",
    "(It is equal because our full set had 100 files in each category; a Stratified Split preserves the relative distribution of instances per category, also if the dataset is unbalanced between the classess.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks\n",
    "\n",
    "A Convolutional Neural Network (ConvNet or CNN) is a type of (deep) Neural Network that is well-suited for 2D axes data, such as images or spectrograms, as it is optimized for learning from spatial proximity. Its core elements are 2D filter kernels which essentially learn the weights of the Neural Network, and downscaling functions such as Max Pooling.\n",
    "\n",
    "A CNN can have one or more Convolution layers, each of them having an arbitrary number of N filters (which define the depth of the CNN layer), following typically by a pooling step, which aggregates neighboring pixels together and thus reduces the image resolution by retaining only the maximum values of neighboring pixels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Data\n",
    "\n",
    "### Adding the channel\n",
    "\n",
    "As CNNs were invented for image data (often having 3 color channels), we need to add a dimension for the color channel to the data. \n",
    "\n",
    "<b>Spectrograms, are considered like greyscale images, which only have 1 color channel. Still we add the extra dimension, defining just 1 channel.</b>\n",
    "\n",
    "In Theano, traditionally the color channel is the <b>first</b> dimension in the image shape. \n",
    "In Tensorflow, the color channel is the <b>last</b> dimension in the image shape. \n",
    "\n",
    "This can be configured now in ~/.keras/keras.json: \"image_dim_ordering\": \"th\" or \"tf\" with \"tf\" (Tensorflow) being the default image ordering even though you use Theano. Depending on this, use one of the code lines below.\n",
    "\n",
    "We created an 'if' statement here to check which dimension ordering to use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_channels = 1 # 1 for grey-scale, 3 for RGB (in this case usually already present in the data)\n",
    "\n",
    "if keras.backend.image_dim_ordering() == 'th':\n",
    "    # Theano ordering (~/.keras/keras.json: \"image_dim_ordering\": \"th\")\n",
    "    train_set = train_set.reshape(train_set.shape[0], n_channels, ydim, xdim)\n",
    "    test_set = test_set.reshape(test_set.shape[0], n_channels, ydim, xdim)\n",
    "else:\n",
    "    # Tensorflow ordering (~/.keras/keras.json: \"image_dim_ordering\": \"tf\")\n",
    "    train_set = train_set.reshape(train_set.shape[0], ydim, xdim, n_channels)\n",
    "    test_set = test_set.reshape(test_set.shape[0], ydim, xdim, n_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tf'"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.backend.image_dim_ordering()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(750, 40, 80, 1)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250, 40, 80, 1)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40, 80, 1)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we store the new shape of the images in the 'input_shape' variable.\n",
    "# take all dimensions except the 0th one (which is the number of images)\n",
    "input_shape = train_set.shape[1:]  \n",
    "input_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Neural Network Models in Keras\n",
    "\n",
    "## Sequential Models\n",
    "\n",
    "In Keras, one can choose between a Sequential model and a Graph model. Sequential models are the standard case. Graph models are for parallel networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Single Layer and a Two Layer CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try: (comment/uncomment code in the following code block)\n",
    "* 1 Layer\n",
    "* 2 Layer\n",
    "* more conv_filters\n",
    "* Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(0) # make results repeatable\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "#conv_filters = 16   # number of convolution filters (= CNN depth)\n",
    "conv_filters = 32   # number of convolution filters (= CNN depth)\n",
    "\n",
    "# Layer 1\n",
    "model.add(Convolution2D(conv_filters, 3, 3, input_shape=input_shape))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2))) \n",
    "#model.add(Dropout(0.25)) \n",
    "\n",
    "# Layer 2\n",
    "model.add(Convolution2D(conv_filters, 3, 3))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2))) \n",
    "\n",
    "# After Convolution, we have a 16*x*y matrix output\n",
    "# In order to feed this to a Full(Dense) layer, we need to flatten all data\n",
    "# Note: Keras does automatic shape inference, i.e. it knows how many (flat) input units the next layer will need,\n",
    "# so no parameter is needed for the Flatten() layer.\n",
    "model.add(Flatten()) \n",
    "\n",
    "# Full layer\n",
    "model.add(Dense(256, activation='sigmoid')) \n",
    "\n",
    "# Output layer\n",
    "# For binary/2-class problems use ONE sigmoid unit, for multi-class/multi-label problems use n output units \n",
    "# activation should be 'softmax' for multi-class / single-label output, 'sigmoid' for binary or multi-label tasks\n",
    "model.add(Dense(n_classes,activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you get OverflowError: Range exceeds valid bounds in the above box, check the correct Theano vs. Tensorflow ordering in the box before and your keras.json configuration file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "convolution2d_8 (Convolution2D)  (None, 38, 78, 32)    320         convolution2d_input_6[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_8 (MaxPooling2D)    (None, 19, 39, 32)    0           convolution2d_8[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_9 (Convolution2D)  (None, 17, 37, 32)    9248        maxpooling2d_8[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_9 (MaxPooling2D)    (None, 8, 18, 32)     0           convolution2d_9[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "flatten_6 (Flatten)              (None, 4608)          0           maxpooling2d_9[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "dense_11 (Dense)                 (None, 256)           1179904     flatten_6[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_12 (Dense)                 (None, 10)            2570        dense_11[0][0]                   \n",
      "====================================================================================================\n",
      "Total params: 1,192,042\n",
      "Trainable params: 1,192,042\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a loss function \n",
    "loss = 'categorical_crossentropy' \n",
    "\n",
    "# Note: for binary classification (2 classes) OR for multi-class problems use:\n",
    "#loss = 'binary_crossentropy' \n",
    "\n",
    "# Optimizer = Stochastic Gradient Descent\n",
    "optimizer = 'sgd' \n",
    "\n",
    "# Compiling the model\n",
    "model.compile(loss=loss, optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "750/750 [==============================] - 5s - loss: 1.6616 - acc: 0.4493     \n",
      "Epoch 2/15\n",
      "750/750 [==============================] - 5s - loss: 1.6386 - acc: 0.4387     \n",
      "Epoch 3/15\n",
      "750/750 [==============================] - 5s - loss: 1.6132 - acc: 0.4533     \n",
      "Epoch 4/15\n",
      "750/750 [==============================] - 5s - loss: 1.5851 - acc: 0.4707     \n",
      "Epoch 5/15\n",
      "750/750 [==============================] - 5s - loss: 1.5588 - acc: 0.4867     \n",
      "Epoch 6/15\n",
      "750/750 [==============================] - 5s - loss: 1.5385 - acc: 0.4880     \n",
      "Epoch 7/15\n",
      "750/750 [==============================] - 5s - loss: 1.5066 - acc: 0.5107     \n",
      "Epoch 8/15\n",
      "750/750 [==============================] - 5s - loss: 1.4819 - acc: 0.5187     \n",
      "Epoch 9/15\n",
      "750/750 [==============================] - 5s - loss: 1.4579 - acc: 0.5280     \n",
      "Epoch 10/15\n",
      "750/750 [==============================] - 5s - loss: 1.4294 - acc: 0.5413     \n",
      "Epoch 11/15\n",
      "750/750 [==============================] - 5s - loss: 1.4039 - acc: 0.5467     \n",
      "Epoch 12/15\n",
      "750/750 [==============================] - 5s - loss: 1.3730 - acc: 0.5707     \n",
      "Epoch 13/15\n",
      "750/750 [==============================] - 5s - loss: 1.3397 - acc: 0.5693     \n",
      "Epoch 14/15\n",
      "750/750 [==============================] - 5s - loss: 1.3210 - acc: 0.5987     \n",
      "Epoch 15/15\n",
      "750/750 [==============================] - 5s - loss: 1.2931 - acc: 0.5933     \n"
     ]
    }
   ],
   "source": [
    "# TRAINING the model\n",
    "\n",
    "# for how many epochs (iterations) to train\n",
    "epochs = 15\n",
    "\n",
    "# for training we need the \"1 hot encoded\" numeric classes of the ground truth\n",
    "history = model.fit(train_set, train_classes_1hot, batch_size=32, nb_epoch=epochs)\n",
    "\n",
    "# we keep the history of accuracies on training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO plot history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy goes up pretty quickly for 1 layer on Train set! Also on Test set?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verifying Accuracy on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 0s     \n"
     ]
    }
   ],
   "source": [
    "test_pred = model.predict_classes(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 2, 4, 2, 1, 7, 4, 4, 1, ..., 8, 5, 1, 6, 2, 6, 2, 6, 5, 7])"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predictions\n",
    "test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 7, 5, 3, 0, 7, 7, 5, 4, 1, ..., 2, 5, 5, 8, 2, 6, 0, 6, 2, 5])"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# groundtruth\n",
    "test_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.34799999999999998"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1 layer\n",
    "accuracy_score(test_classes, test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.36799999999999999"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2 layer\n",
    "accuracy_score(test_classes, test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.71875"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2 layer + 32 convolution filters\n",
    "accuracy_score(test_classes, test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.71875"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2 layer + 32 convolution filters + ReLU + Dropout\n",
    "accuracy_score(test_classes, test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "conv_filters = 16   # number of convolution filters (= CNN depth)\n",
    "\n",
    "cnn_act = 'sigmoid'\n",
    "\n",
    "# Layer 1\n",
    "#model.add(Convolution2D(conv_filters, 3, 3, border_mode='valid', input_shape=input_shape))\n",
    "model.add(Convolution2D(conv_filters, 4, 12, activation=cnn_act, border_mode='valid', input_shape=input_shape))\n",
    "#model.add(MaxPooling2D(pool_size=(2, 2))) \n",
    "model.add(MaxPooling2D(pool_size=(1, 20))) \n",
    "\n",
    "# Layer 2\n",
    "#model.add(Convolution2D(conv_filters, 3, 3, border_mode='valid', input_shape=input_shape))\n",
    "#model.add(BatchNormalization())\n",
    "#model.add(Activation('relu')) \n",
    "#model.add(MaxPooling2D(pool_size=(2, 2))) \n",
    "\n",
    "# After Convolution, we have a 16*x*y matrix output\n",
    "# In order to feed this to a Full(Dense) layer, we need to flatten all data\n",
    "# Note: Keras does automatic shape inference, i.e. it knows how many (flat) input units the next layer will need,\n",
    "# so no parameter is needed for the Flatten() layer.\n",
    "model.add(Flatten()) \n",
    "\n",
    "# Full layer\n",
    "model.add(Dense(200, activation='sigmoid'))  \n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# Output layer\n",
    "# For binary/2-class problems use ONE sigmoid unit, for multi-class/multi-label problems use n output units \n",
    "# activation should be 'softmax' for multi-class / single-label output, 'sigmoid' for binary or multi-label tasks\n",
    "model.add(Dense(n_classes,activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compiling the model\n",
    "\n",
    "learn_rate = 0.05\n",
    "optimizer = SGD(lr=learn_rate) #, momentum=momentum) \n",
    "\n",
    "model.compile(loss='categorical_crossentropy' , optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "750/750 [==============================] - 2s - loss: 1.1851 - acc: 0.5840     \n",
      "Epoch 2/30\n",
      "750/750 [==============================] - 2s - loss: 1.1653 - acc: 0.5813     \n",
      "Epoch 3/30\n",
      "750/750 [==============================] - 2s - loss: 1.1848 - acc: 0.5627     \n",
      "Epoch 4/30\n",
      "750/750 [==============================] - 1s - loss: 1.1689 - acc: 0.5853     \n",
      "Epoch 5/30\n",
      "750/750 [==============================] - 1s - loss: 1.1437 - acc: 0.6027     \n",
      "Epoch 6/30\n",
      "750/750 [==============================] - 2s - loss: 1.1328 - acc: 0.5987     \n",
      "Epoch 7/30\n",
      "750/750 [==============================] - 1s - loss: 1.1565 - acc: 0.5987     \n",
      "Epoch 8/30\n",
      "750/750 [==============================] - 1s - loss: 1.1089 - acc: 0.6240     \n",
      "Epoch 9/30\n",
      "750/750 [==============================] - 1s - loss: 1.1030 - acc: 0.6147     \n",
      "Epoch 10/30\n",
      "750/750 [==============================] - 1s - loss: 1.0911 - acc: 0.6067     \n",
      "Epoch 11/30\n",
      "750/750 [==============================] - 1s - loss: 1.1006 - acc: 0.6173     \n",
      "Epoch 12/30\n",
      "750/750 [==============================] - 1s - loss: 1.0820 - acc: 0.6080     \n",
      "Epoch 13/30\n",
      "750/750 [==============================] - 1s - loss: 1.0728 - acc: 0.6213     \n",
      "Epoch 14/30\n",
      "750/750 [==============================] - 1s - loss: 1.0535 - acc: 0.6267     \n",
      "Epoch 15/30\n",
      "750/750 [==============================] - 1s - loss: 1.0680 - acc: 0.6240     \n",
      "Epoch 16/30\n",
      "750/750 [==============================] - 1s - loss: 1.0457 - acc: 0.6547     \n",
      "Epoch 17/30\n",
      "750/750 [==============================] - 1s - loss: 1.0408 - acc: 0.6427     \n",
      "Epoch 18/30\n",
      "750/750 [==============================] - 1s - loss: 1.0578 - acc: 0.6240     \n",
      "Epoch 19/30\n",
      "750/750 [==============================] - 2s - loss: 1.0270 - acc: 0.6200     \n",
      "Epoch 20/30\n",
      "750/750 [==============================] - 2s - loss: 1.0183 - acc: 0.6373     \n",
      "Epoch 21/30\n",
      "750/750 [==============================] - 1s - loss: 0.9944 - acc: 0.6507     \n",
      "Epoch 22/30\n",
      "750/750 [==============================] - 2s - loss: 1.0143 - acc: 0.6333     \n",
      "Epoch 23/30\n",
      "750/750 [==============================] - 2s - loss: 1.0022 - acc: 0.6440     \n",
      "Epoch 24/30\n",
      "750/750 [==============================] - 2s - loss: 0.9828 - acc: 0.6573     \n",
      "Epoch 25/30\n",
      "750/750 [==============================] - 2s - loss: 0.9621 - acc: 0.6573     \n",
      "Epoch 26/30\n",
      "750/750 [==============================] - 2s - loss: 0.9760 - acc: 0.6400     \n",
      "Epoch 27/30\n",
      "750/750 [==============================] - 2s - loss: 0.9695 - acc: 0.6480     \n",
      "Epoch 28/30\n",
      "750/750 [==============================] - 2s - loss: 0.9452 - acc: 0.6640     \n",
      "Epoch 29/30\n",
      "750/750 [==============================] - 2s - loss: 0.9431 - acc: 0.6667     \n",
      "Epoch 30/30\n",
      "750/750 [==============================] - 2s - loss: 0.9323 - acc: 0.6627     \n"
     ]
    }
   ],
   "source": [
    "# TRAINING the model - (you may repeat execution of this cell to FURTHER train the model!)\n",
    "epochs = 30\n",
    "history = model.fit(train_set, train_classes_1hot, batch_size=32, nb_epoch=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 0s     \n"
     ]
    }
   ],
   "source": [
    "test_pred = model.predict_classes(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.496"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(test_classes, test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Parameters & Techniques\n",
    "\n",
    "Try: (comment/uncomment code blocks below)\n",
    "* Adding ReLU activation\n",
    "* Adding Batch normalization\n",
    "* Adding Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The shape of the input to \"Flatten\" is not fully defined (got (17, 0, 16). Make sure to pass a complete \"input_shape\" or \"batch_input_shape\" argument to the first layer in your model.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-257-c76d99915793>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# Note: Keras does automatic shape inference, i.e. it knows how many (flat) input units the next layer will need,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# so no parameter is needed for the Flatten() layer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFlatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;31m# Full layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/keras/models.pyc\u001b[0m in \u001b[0;36madd\u001b[0;34m(self, layer)\u001b[0m\n\u001b[1;32m    330\u001b[0m                  output_shapes=[self.outputs[0]._keras_shape])\n\u001b[1;32m    331\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m             \u001b[0moutput_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m                 raise TypeError('All layers in a Sequential model '\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/keras/engine/topology.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m    570\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minbound_layers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m             \u001b[0;31m# This will call layer.build() if necessary.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 572\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_inbound_node\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minbound_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    573\u001b[0m             \u001b[0;31m# Outputs were already computed when calling self.add_inbound_node.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minbound_nodes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_tensors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/keras/engine/topology.pyc\u001b[0m in \u001b[0;36madd_inbound_node\u001b[0;34m(self, inbound_layers, node_indices, tensor_indices)\u001b[0m\n\u001b[1;32m    633\u001b[0m         \u001b[0;31m# creating the node automatically updates self.inbound_nodes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m         \u001b[0;31m# as well as outbound_nodes on inbound layers.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 635\u001b[0;31m         \u001b[0mNode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_node\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minbound_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_output_shape_for\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/keras/engine/topology.pyc\u001b[0m in \u001b[0;36mcreate_node\u001b[0;34m(cls, outbound_layer, inbound_layers, node_indices, tensor_indices)\u001b[0m\n\u001b[1;32m    168\u001b[0m             \u001b[0;31m# TODO: try to auto-infer shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m             \u001b[0;31m# if exception is raised by get_output_shape_for.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m             \u001b[0moutput_shapes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutbound_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_output_shape_for\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0moutput_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutbound_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/keras/layers/core.pyc\u001b[0m in \u001b[0;36mget_output_shape_for\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m    473\u001b[0m             raise ValueError('The shape of the input to \"Flatten\" '\n\u001b[1;32m    474\u001b[0m                              \u001b[0;34m'is not fully defined '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m                              \u001b[0;34m'(got '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m                              \u001b[0;34m'Make sure to pass a complete \"input_shape\" '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m                              \u001b[0;34m'or \"batch_input_shape\" argument to the first '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The shape of the input to \"Flatten\" is not fully defined (got (17, 0, 16). Make sure to pass a complete \"input_shape\" or \"batch_input_shape\" argument to the first layer in your model."
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "conv_filters = 16   # number of convolution filters (= CNN depth)\n",
    "\n",
    "# Layer 1\n",
    "model.add(Convolution2D(conv_filters, 3, 3, border_mode='valid', input_shape=input_shape))\n",
    "#model.add(Convolution2D(conv_filters, 4, 12, border_mode='valid', input_shape=input_shape))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2))) \n",
    "#model.add(MaxPooling2D(pool_size=(1, 20))) \n",
    "\n",
    "# Layer 2\n",
    "model.add(Convolution2D(conv_filters, 3, 3, border_mode='valid', input_shape=input_shape))\n",
    "#model.add(BatchNormalization())\n",
    "model.add(Activation('relu')) \n",
    "model.add(MaxPooling2D(pool_size=(2, 2))) \n",
    "\n",
    "# After Convolution, we have a 16*x*y matrix output\n",
    "# In order to feed this to a Full(Dense) layer, we need to flatten all data\n",
    "# Note: Keras does automatic shape inference, i.e. it knows how many (flat) input units the next layer will need,\n",
    "# so no parameter is needed for the Flatten() layer.\n",
    "model.add(Flatten()) \n",
    "\n",
    "# Full layer\n",
    "model.add(Dense(256))  \n",
    "#model.add(Activation('relu'))\n",
    "#model.add(Dropout(0.1))\n",
    "\n",
    "# Output layer\n",
    "# For binary/2-class problems use ONE sigmoid unit, \n",
    "# for multi-class/multi-label problems use n output units and activation='softmax!'\n",
    "model.add(Dense(n_classes,activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compiling the model\n",
    "\n",
    "optimizer = 'sgd'\n",
    "\n",
    "#learn_rate = 0.02\n",
    "#optimizer = SGD(lr=learn_rate) #, momentum=momentum) \n",
    "\n",
    "model.compile(loss='categorical_crossentropy' , optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "750/750 [==============================] - 2s - loss: 2.1835 - acc: 0.1907     \n",
      "Epoch 2/30\n",
      "750/750 [==============================] - 1s - loss: 2.1903 - acc: 0.1987     \n",
      "Epoch 3/30\n",
      "750/750 [==============================] - 1s - loss: 2.1851 - acc: 0.1907     \n",
      "Epoch 4/30\n",
      "750/750 [==============================] - 1s - loss: 2.1793 - acc: 0.1720     \n",
      "Epoch 5/30\n",
      "750/750 [==============================] - 1s - loss: 2.1544 - acc: 0.1947     \n",
      "Epoch 6/30\n",
      "750/750 [==============================] - 1s - loss: 2.1471 - acc: 0.2147     \n",
      "Epoch 7/30\n",
      "750/750 [==============================] - 2s - loss: 2.1230 - acc: 0.2160     \n",
      "Epoch 8/30\n",
      "750/750 [==============================] - 2s - loss: 2.1316 - acc: 0.2107     \n",
      "Epoch 9/30\n",
      "750/750 [==============================] - 1s - loss: 2.0883 - acc: 0.2333     \n",
      "Epoch 10/30\n",
      "750/750 [==============================] - 1s - loss: 2.1306 - acc: 0.2093     \n",
      "Epoch 11/30\n",
      "750/750 [==============================] - 2s - loss: 2.0974 - acc: 0.2320     \n",
      "Epoch 12/30\n",
      "750/750 [==============================] - 1s - loss: 2.0957 - acc: 0.2147     \n",
      "Epoch 13/30\n",
      "750/750 [==============================] - 2s - loss: 2.0829 - acc: 0.2280     \n",
      "Epoch 14/30\n",
      "750/750 [==============================] - 1s - loss: 2.0889 - acc: 0.2120     \n",
      "Epoch 15/30\n",
      "750/750 [==============================] - 2s - loss: 2.0845 - acc: 0.2360     \n",
      "Epoch 16/30\n",
      "750/750 [==============================] - 1s - loss: 2.0732 - acc: 0.2520     \n",
      "Epoch 17/30\n",
      "750/750 [==============================] - 1s - loss: 2.0715 - acc: 0.2333     \n",
      "Epoch 18/30\n",
      "750/750 [==============================] - 1s - loss: 2.0732 - acc: 0.2413     \n",
      "Epoch 19/30\n",
      "750/750 [==============================] - 1s - loss: 2.0486 - acc: 0.2333     \n",
      "Epoch 20/30\n",
      "750/750 [==============================] - 1s - loss: 2.0377 - acc: 0.2600     \n",
      "Epoch 21/30\n",
      "750/750 [==============================] - 1s - loss: 2.0417 - acc: 0.2453     \n",
      "Epoch 22/30\n",
      "750/750 [==============================] - 1s - loss: 2.0302 - acc: 0.2520     \n",
      "Epoch 23/30\n",
      "750/750 [==============================] - 2s - loss: 2.0209 - acc: 0.2600     \n",
      "Epoch 24/30\n",
      "750/750 [==============================] - 1s - loss: 2.0158 - acc: 0.2720     \n",
      "Epoch 25/30\n",
      "750/750 [==============================] - 1s - loss: 1.9985 - acc: 0.2747     \n",
      "Epoch 26/30\n",
      "750/750 [==============================] - 1s - loss: 2.0257 - acc: 0.2827     \n",
      "Epoch 27/30\n",
      "750/750 [==============================] - 2s - loss: 2.0140 - acc: 0.2653     \n",
      "Epoch 28/30\n",
      "750/750 [==============================] - 2s - loss: 1.9995 - acc: 0.2720     \n",
      "Epoch 29/30\n",
      "750/750 [==============================] - 1s - loss: 1.9852 - acc: 0.2720     \n",
      "Epoch 30/30\n",
      "750/750 [==============================] - 1s - loss: 1.9890 - acc: 0.2933     \n"
     ]
    }
   ],
   "source": [
    "# TRAINING the model - (you may repeat execution of this cell to FURTHER train the model!)\n",
    "epochs = 30\n",
    "history = model.fit(train_set, train_classes_1hot, batch_size=32, nb_epoch=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 0s     \n"
     ]
    }
   ],
   "source": [
    "test_pred = model.predict_classes(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.32400000000000001"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(test_classes, test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Parallel CNNs\n",
    "\n",
    "The motivation to use parallel CNNs is to use one CNN that detects patterns in the <b>frequencies</b> and another one that captures patterns in the <b>time domain, i.e. rhythm</b> in a combined way to learn better the structures in the sound.\n",
    "\n",
    "To create parallel CNNs we need a \"graph-based\" model. In Keras 1.x this is realized via the functional API of the Model() class.\n",
    "We use it to create two CNN layers that run in parallel to each other and are merged subsequently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Input only specifies the input shape\n",
    "input = Input(input_shape)\n",
    "\n",
    "# CNN layers\n",
    "# specify desired number of filters\n",
    "n_filters = 32 #16 \n",
    "\n",
    "# The functional API allows to specify the predecessor in (brackets) after the new Layer function call\n",
    "conv_layer1 = Convolution2D(n_filters, 10, 2, activation='relu')(input)  # a vertical filter\n",
    "conv_layer2 = Convolution2D(n_filters, 2, 10, activation='relu')(input)  # a horizontal filter\n",
    "\n",
    "\n",
    "# Pooling layers - equal sized\n",
    "#maxpool1 = MaxPooling2D(pool_size=(2,2))(conv_layer1)\n",
    "#maxpool2 = MaxPooling2D(pool_size=(2,2))(conv_layer2)\n",
    "\n",
    "# ALTERNATIVE: Pooling layers - complementary to vertical/horizontal filter\n",
    "maxpool1 = MaxPooling2D(pool_size=(1,2))(conv_layer1)\n",
    "maxpool2 = MaxPooling2D(pool_size=(2,1))(conv_layer2)\n",
    "\n",
    "\n",
    "# we have to flatten the Pooling output in order to be concatenated\n",
    "poolflat1 = Flatten()(maxpool1)\n",
    "poolflat2 = Flatten()(maxpool2)\n",
    "\n",
    "# Merge the 2\n",
    "merged = merge([poolflat1, poolflat2], mode='concat')\n",
    "\n",
    "full = Dense(256, activation='sigmoid')(merged)\n",
    "output_layer = Dense(n_classes, activation='softmax')(full)\n",
    "\n",
    "# finally create the model\n",
    "model = Model(input=input, output=output_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_6 (InputLayer)             (None, 40, 80, 1)     0                                            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_28 (Convolution2D) (None, 31, 79, 32)    672         input_6[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_29 (Convolution2D) (None, 39, 71, 32)    672         input_6[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_28 (MaxPooling2D)   (None, 31, 39, 32)    0           convolution2d_28[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_29 (MaxPooling2D)   (None, 19, 71, 32)    0           convolution2d_29[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "flatten_21 (Flatten)             (None, 38688)         0           maxpooling2d_28[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "flatten_22 (Flatten)             (None, 43168)         0           maxpooling2d_29[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "merge_6 (Merge)                  (None, 81856)         0           flatten_21[0][0]                 \n",
      "                                                                   flatten_22[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dense_31 (Dense)                 (None, 256)           20955392    merge_6[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_32 (Dense)                 (None, 10)            2570        dense_31[0][0]                   \n",
      "====================================================================================================\n",
      "Total params: 20,959,306\n",
      "Trainable params: 20,959,306\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compiling the model\n",
    "\n",
    "learn_rate = 0.02\n",
    "optimizer = SGD(lr=learn_rate) #, momentum=momentum) \n",
    "\n",
    "model.compile(loss='categorical_crossentropy' , optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "750/750 [==============================] - 11s - loss: 2.1489 - acc: 0.2173    \n",
      "Epoch 2/15\n",
      "750/750 [==============================] - 11s - loss: 1.9068 - acc: 0.3107    \n",
      "Epoch 3/15\n",
      "750/750 [==============================] - 10s - loss: 1.7642 - acc: 0.3960    \n",
      "Epoch 4/15\n",
      "750/750 [==============================] - 11s - loss: 1.6289 - acc: 0.4667    \n",
      "Epoch 5/15\n",
      "750/750 [==============================] - 12s - loss: 1.5371 - acc: 0.4840    \n",
      "Epoch 6/15\n",
      "750/750 [==============================] - 13s - loss: 1.4501 - acc: 0.4920    \n",
      "Epoch 7/15\n",
      "750/750 [==============================] - 13s - loss: 1.3461 - acc: 0.5547    \n",
      "Epoch 8/15\n",
      "750/750 [==============================] - 12s - loss: 1.2918 - acc: 0.5693    \n",
      "Epoch 9/15\n",
      "750/750 [==============================] - 10s - loss: 1.1987 - acc: 0.6133    \n",
      "Epoch 10/15\n",
      "750/750 [==============================] - 10s - loss: 1.1592 - acc: 0.6200    \n",
      "Epoch 11/15\n",
      "750/750 [==============================] - 10s - loss: 1.1185 - acc: 0.6280    \n",
      "Epoch 12/15\n",
      "750/750 [==============================] - 10s - loss: 1.0509 - acc: 0.6507    \n",
      "Epoch 13/15\n",
      "750/750 [==============================] - 10s - loss: 0.9819 - acc: 0.6933    \n",
      "Epoch 14/15\n",
      "750/750 [==============================] - 10s - loss: 0.9241 - acc: 0.7147    \n",
      "Epoch 15/15\n",
      "750/750 [==============================] - 10s - loss: 0.8636 - acc: 0.7333    \n"
     ]
    }
   ],
   "source": [
    "# TRAINING the model\n",
    "epochs = 15\n",
    "history = model.fit(train_set, train_classes_1hot, batch_size=32, nb_epoch=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#test_pred = model.predict_classes(test_set)\n",
    "\n",
    "# THE GRAPH MODEL DOES NOT SUPPORT .predict_classes\n",
    "# we use model.predict\n",
    "\n",
    "test_pred = model.predict(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00000088,  0.99960399,  0.0000328 ,  0.00000023,  0.        ,  0.00035961,  0.        ,  0.00000055,  0.00000029,  0.00000165],\n",
       "       [ 0.1437121 ,  0.03942114,  0.08495273,  0.05749788,  0.00098327,  0.43678114,  0.00377144,  0.05442544,  0.1472127 ,  0.03124217],\n",
       "       [ 0.1809573 ,  0.01344254,  0.19387296,  0.04931471,  0.007558  ,  0.16700505,  0.00760951,  0.00200711,  0.06899887,  0.30923396],\n",
       "       [ 0.00307811,  0.00009272,  0.0044343 ,  0.13793397,  0.51605999,  0.18190596,  0.02145181,  0.02988708,  0.06580672,  0.03934935],\n",
       "       [ 0.20178558,  0.21113855,  0.08383352,  0.18441004,  0.00160872,  0.02205589,  0.01210712,  0.03152585,  0.02146892,  0.23006581],\n",
       "       [ 0.20334674,  0.03489076,  0.01057608,  0.02076097,  0.0004238 ,  0.06517673,  0.02537389,  0.00362619,  0.48641843,  0.14940642],\n",
       "       [ 0.00178482,  0.00014169,  0.12583028,  0.03404034,  0.00496965,  0.01788303,  0.00012243,  0.80130893,  0.01154513,  0.00237368],\n",
       "       [ 0.00540129,  0.0000861 ,  0.77838236,  0.03259563,  0.03244663,  0.00260737,  0.06765364,  0.06994878,  0.00177144,  0.00910675],\n",
       "       [ 0.00157264,  0.00014476,  0.00087443,  0.63497156,  0.03262713,  0.20330653,  0.00168838,  0.00039969,  0.00274102,  0.12167384],\n",
       "       [ 0.00279802,  0.71088147,  0.0374315 ,  0.04042223,  0.00199453,  0.17104498,  0.000462  ,  0.0025956 ,  0.01632562,  0.01604406],\n",
       "       ..., \n",
       "       [ 0.01258961,  0.03639963,  0.07114083,  0.0100976 ,  0.00641961,  0.06121289,  0.01064838,  0.00378162,  0.68290466,  0.10480516],\n",
       "       [ 0.00194605,  0.11142774,  0.00701647,  0.0036546 ,  0.00027583,  0.84707069,  0.00115797,  0.00005337,  0.0063235 ,  0.02107379],\n",
       "       [ 0.00276692,  0.28381634,  0.02161958,  0.27993828,  0.00619074,  0.34911478,  0.00376337,  0.00291991,  0.0282206 ,  0.02164947],\n",
       "       [ 0.00010893,  0.00000429,  0.00000203,  0.00091748,  0.00003237,  0.0000013 ,  0.99864215,  0.0000089 ,  0.00002611,  0.00025647],\n",
       "       [ 0.07167248,  0.0371546 ,  0.08135408,  0.02031563,  0.01966758,  0.02861126,  0.02253592,  0.02622466,  0.66229969,  0.03016412],\n",
       "       [ 0.00038857,  0.00000008,  0.00018161,  0.00052266,  0.00008245,  0.00000141,  0.99827957,  0.00007121,  0.00000369,  0.00046874],\n",
       "       [ 0.11903777,  0.01035502,  0.73243141,  0.04416187,  0.00021958,  0.00183885,  0.00215801,  0.0041567 ,  0.01177309,  0.07386769],\n",
       "       [ 0.00399152,  0.00000029,  0.00100982,  0.00007966,  0.00045716,  0.00000609,  0.99356133,  0.00000445,  0.00000348,  0.00088619],\n",
       "       [ 0.11188012,  0.03455363,  0.51501399,  0.00667377,  0.00685032,  0.01637956,  0.00237979,  0.045604  ,  0.19668414,  0.06398069],\n",
       "       [ 0.00131133,  0.00003892,  0.01000302,  0.45234931,  0.00242855,  0.00271313,  0.0024455 ,  0.4679336 ,  0.04109688,  0.01967977]], dtype=float32)"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# it predicts the probabilities per class:\n",
    "test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 5, 9, 4, 9, 8, 7, 2, 3, 1, ..., 8, 5, 5, 6, 8, 6, 2, 6, 2, 7])"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we get the predicted class from the maximum value in the probabilities, using argmax to get the class number\n",
    "test_pred = np.argmax(test_pred, axis=1)\n",
    "test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.47999999999999998"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(test_classes, test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Practice\n",
    "\n",
    "* add more layers\n",
    "* vary:\n",
    " * number of filters\n",
    " * filter sizes (shapes)\n",
    " * max pooling\n",
    " * activation functions: sigmoid, tanh, relu, prelu, elu, ...\n",
    "* try other optimizers: 'from keras.optimizers import SGD, RMSprop, Adagrad'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
